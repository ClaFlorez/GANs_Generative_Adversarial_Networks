# ğŸ¨ DCGAN â€” RÃ©seau GÃ©nÃ©ratif Adversarial Convolutif sur CIFAR-10
*Documentation technique et guide dâ€™utilisation â€“ en franÃ§ais*

## 1. Introduction
Les **GANs** (*Generative Adversarial Networks*) permettent de crÃ©er de nouvelles images Ã  partir dâ€™un bruit alÃ©atoire.  
Un **DCGAN** (*Deep Convolutional GAN*) est une version amÃ©liorÃ©e adaptÃ©e aux images en couleur grÃ¢ce Ã  des couches de convolution et de dÃ©convolution.  

### ğŸ¯ Objectif du projet
GÃ©nÃ©rer des images **RVB (32Ã—32)** inspirÃ©es du jeu de donnÃ©es **CIFAR-10** Ã  lâ€™aide dâ€™un gÃ©nÃ©rateur et dâ€™un discriminateur entraÃ®nÃ©s en compÃ©tition.  

---

## 2. Structure du notebook
Le notebook se compose de plusieurs sections :

| Section | Description |
|----------|--------------|
| Importations | Chargement de PyTorch, torchvision et utilitaires. |
| Configuration | DÃ©finition des hyperparamÃ¨tres (taille du lot, taux dâ€™apprentissage, nombre dâ€™Ã©poques, etc.). |
| Dataset | Chargement de CIFAR-10 et normalisation dans lâ€™intervalle [-1, 1]. |
| ModÃ¨les | DÃ©finition des classes `Generator` et `Discriminator`. |
| Boucle dâ€™entraÃ®nement | Mise Ã  jour alternÃ©e des rÃ©seaux G et D. |
| Visualisation | Sauvegarde et affichage des Ã©chantillons Ã  chaque Ã©poque. |
| Sauvegarde des modÃ¨les | Enregistrement des poids au format `.pth`. |

---

## 3. Explication technique dÃ©taillÃ©e  

### ğŸ§© Architecture du GÃ©nÃ©rateur  
Le **gÃ©nÃ©rateur (G)** reÃ§oit un vecteur de bruit `z` (100 valeurs alÃ©atoires) et le transforme en image RGB (3 canaux, 64Ã—64) par une succession de **couches de convolution transposÃ©e** :

```
z (100,1,1)
 â”œâ”€ ConvTranspose2d â†’ 4x4x512
 â”œâ”€ BatchNorm2d + ReLU
 â”œâ”€ ConvTranspose2d â†’ 8x8x256
 â”œâ”€ ConvTranspose2d â†’ 16x16x128
 â”œâ”€ ConvTranspose2d â†’ 32x32x64
 â”œâ”€ ConvTranspose2d â†’ 64x64x3
 â””â”€ Tanh â†’ image finale [-1,1]
```

Chaque couche agrandit spatialement lâ€™image et affine les dÃ©tails.  
La fonction **Tanh** assure que la sortie soit normalisÃ©e dans \([-1,1]\), correspondant Ã  la normalisation du dataset.  

### ğŸ” Architecture du Discriminateur  
Le **discriminateur (D)** reÃ§oit une image (rÃ©elle ou gÃ©nÃ©rÃ©e) et apprend Ã  distinguer les vraies des fausses :

```
Image (3,64,64)
 â”œâ”€ Conv2d â†’ 32x32x64
 â”œâ”€ LeakyReLU(0.2)
 â”œâ”€ Conv2d â†’ 16x16x128
 â”œâ”€ BatchNorm2d + LeakyReLU
 â”œâ”€ Conv2d â†’ 8x8x256
 â”œâ”€ Conv2d â†’ 4x4x512
 â”œâ”€ Conv2d â†’ 1x1x1
 â””â”€ Sigmoid â†’ probabilitÃ© "rÃ©elle"
```

Le **LeakyReLU** Ã©vite le problÃ¨me du *dead ReLU*, et la **Sigmoid** produit une probabilitÃ© entre 0 et 1.  

### âš–ï¸ Fonction de perte  
Le DCGAN optimise un **jeu min-max** entre les deux rÃ©seaux :

\[
\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p_z}[\log(1 - D(G(z)))]
\]

- Le **discriminateur** maximise la probabilitÃ© dâ€™identifier correctement les vraies images.  
- Le **gÃ©nÃ©rateur** cherche Ã  minimiser la capacitÃ© de D Ã  les distinguer.  

### ğŸ”§ Optimisateurs  
Utilisation dâ€™**Adam** avec `lr=0.0002`, `betas=(0.5, 0.999)` pour une convergence stable.

---

## 4. Guide dâ€™utilisation (Google Colab)  

### âš™ï¸ Ã‰tapes principales  
1. **Importer le notebook `.ipynb` dans Google Colab**.  
2. Aller dans le menu **ExÃ©cution â†’ Modifier le type dâ€™exÃ©cution â†’ GPU**.  
3. Lancer les cellules dans lâ€™ordre :  
   - Installation et importations  
   - Chargement du dataset  
   - DÃ©finition du modÃ¨le  
   - EntraÃ®nement  
4. Ã€ chaque Ã©poque, une image `epoch_XXX.png` sâ€™affiche dans `/content/samples_color_gan/`.  

### ğŸ’¾ Sauvegarde des modÃ¨les  
Les poids sont enregistrÃ©s sous :  
```
checkpoints/G_epoch_XXX.pth
checkpoints/D_epoch_XXX.pth
```

Ces fichiers peuvent Ãªtre rechargÃ©s pour gÃ©nÃ©rer de nouvelles images sans rÃ©-entraÃ®ner le rÃ©seau.  

---

## 5. GÃ©nÃ©rer de nouvelles images avec un modÃ¨le sauvegardÃ©  

```python
import torch
from torchvision.utils import save_image, make_grid
from matplotlib import pyplot as plt
from dcgan_color import Generator  # importer la mÃªme classe que celle de lâ€™entraÃ®nement

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
G = Generator(z_dim=100, channels=3)
G.load_state_dict(torch.load("checkpoints/G_epoch_025.pth", map_location=device))
G.eval()

z = torch.randn(64, 100, 1, 1, device=device)
with torch.no_grad():
    fake = G(z).cpu()

grid = make_grid((fake + 1) / 2, nrow=8)
save_image(grid, "images_generees.png")

plt.figure(figsize=(6,6))
plt.axis("off")
plt.imshow(grid.permute(1,2,0))
plt.show()
```

---

## 6. InterprÃ©tation des rÃ©sultats  
- **Loss_D** : diminue quand le discriminateur apprend Ã  dÃ©tecter les fausses images.  
- **Loss_G** : diminue quand le gÃ©nÃ©rateur produit des images plus rÃ©alistes.  
- Les premiers Ã©chantillons sont flous ; aprÃ¨s une dizaine dâ€™Ã©poques, on observe des contours et des couleurs plausibles.  

### ğŸï¸ Visualisation complÃ¨te  
Un GIF dâ€™Ã©volution peut Ãªtre gÃ©nÃ©rÃ© avec :

```python
import imageio, glob
frames = [imageio.imread(p) for p in sorted(glob.glob(f"{out_dir}/epoch_*.png"))]
imageio.mimsave(f"{out_dir}/evolution_cifar10.gif", frames, fps=2)
```

---

## 7. AmÃ©liorations possibles  
- **WGAN-GP** pour une stabilitÃ© renforcÃ©e (perte de Wasserstein).  
- **Spectral Normalization** pour un contrÃ´le des gradients.  
- **Augmentation** lÃ©gÃ¨re du dataset (flip, rotation).  
- **StyleGAN** pour un contrÃ´le fin du style et de la variabilitÃ©.  
